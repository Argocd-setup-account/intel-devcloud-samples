{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to DL Streamer\n",
    "[Deep Learning(DL) Streamer](https://github.com/opencv/gst-video-analytics) is an easy way to construct media analytics pipelines using OpenVINO™. It leverages the open source media framework [GStreamer](https://gstreamer.freedesktop.org/) to provide optimized media operations and OpenVINO™ to provide optimized inference.\n",
    "The elements packaged in the DL Streamer binary release can be divided into three categories:\n",
    "- Elements for optimized streaming media operations (usb and ip camera support, file handling, decode, color-space-conversion, scaling, encoding, rendering, etc.). These elements are developed by the larger GStreamer community.\n",
    "\n",
    "- Elements that integrate the OpenVINO™ inference engine for optimized video analytics (detection, classification, tracking). These elements are provided as part of the OpenVINO™ toolkit.\n",
    "\n",
    "- Elements that convert and publish inference results to the screen as overlaid bounding boxes, to a file as a list of JSON Objects, or to popular message brokers (Kafka or MQTT) as JSON messages. These elements are provided as part of the OpenVINO™ toolkit.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenVINO version check:\n",
    "You are currently using the Intel® Distribution of OpenVINO™ Toolkit LTS version. Alternatively, you can open a version of this notebook for the Intel® Distribution of OpenVINO™ Toolkit latest development version by [clicking this link](../../../openvino-dev-latest/tutorials/DLStreamer/DL_Streamer_Benchmark.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial Overview\n",
    "[Section 1](#about-gstreamer): we will introduce the basic GStreamer concept by defining some terms that you will see and giving a few examples.  \n",
    "[Section 2](#setup): This is where we will setup for the final two sections. We will go over DevCloud concepts like viewing target platforms as well as do some preparation for inferencing by downloading OpenVino models and preparing what DL Streamer calls a model_proc file.  \n",
    "[Section 3](#vehicle-classification): We will create our first pipeline and perform vehicle detection and classification. In this section you will learn to use DL Streamer for inferencing and view the results overlayed onto the video.  \n",
    "[Section 4](#benchmarking): Finally we will look at performance of our inferencing pipelines on various Intel platforms as well as show how you can improve your performance. We will modify the pipeline from section 3 to measure its FPS and then add tracking to see how it can be improved.\n",
    "\n",
    "Once you complete this tutorial you will be able to quickly prototype media analytics pipelines, evaluate their performance on Intel® platforms and understand some of the configuration options from DL Streamer that can affect performance.\n",
    "\n",
    "<a id='about-gstreamer'></a>\n",
    "## About GStreamer\n",
    "In this section we introduce basic GStreamer concepts that you will use in the rest of the tutorial. If you are already familiar with GStreamer feel free to skip ahead to [Section 2 'Setup'](#setup).  \n",
    "\n",
    "[GStreamer](https://gstreamer.freedesktop.org/) is a flexible, fast and multiplatform open-source multimedia framework. It has an easy to use command line tool for running  pipelines, as well as an API with bindings in C, Python, Javascript and [more](https://gstreamer.freedesktop.org/bindings/).\n",
    "In this tutorial we will use the GStreamer command line tool `gst-launch-1.0`. For more information and examples please refer to the online documentation [gst-launch-1.0](https://gstreamer.freedesktop.org/documentation/tools/gst-launch.html?gi-language=c).  \n",
    "\n",
    "### Pipelines\n",
    "The command line tool `gst-launch-1.0` enables developers to describe media analytics pipeline as a series of connected elements. The list of elements, their configuration properties, and their connections are all specified as a list of strings seperated by exclamation marks (!). `gst-launch-1.0` parses the string and instantiates the software modules which perform the individual media analytics operations. Internally the GStreamer library constructs a pipeline object that contains the individual elements and handles common operations such as clocking, messaging, and state changes.\n",
    "\n",
    "**Example**:\n",
    "```gst-launch-1.0 videotestsrc ! ximagesink```\n",
    "\n",
    "### Elements\n",
    "An [element](https://gstreamer.freedesktop.org/documentation/application-development/basics/elements.html?gi-language=c) is the fundamental building block of a pipeline. Elements perform specific operations on incoming frames and then push the resulting frames downstream for further processing. Elements are linked together textually by exclamation marks (`!`) with the full chain of elements representing the entire pipeline. Each element will take data from its upstream element, process it and then output the data for processing by the next element.\n",
    "\n",
    "Elements designated as **source** elements provide input into the pipeline from external sources. In this tutorial we use the [filesrc](https://gstreamer.freedesktop.org/documentation/coreelements/filesrc.html?gi-language=c#filesrc) element that reads input from a local file.  \n",
    "\n",
    "Elements designated as **sink** elements represent the final stage of a pipeline. As an example, a sink element could write transcoded frames to a file on the local disk or open a window to render the video content to the screen or even restream the content via rtsp. In the benchmarking section of this tutorial our primary focus will be to compare the performance of media analytics pipelines on different types of hardware and so we will use the standard [fakesink](https://gstreamer.freedesktop.org/documentation/coreelements/fakesink.html?gi-language=c#fakesink) element to end the pipeline immediately after the analytics is complete without further processing.\n",
    "\n",
    "We will also use the [decodebin](https://gstreamer.freedesktop.org/documentation/playback/decodebin.html#decodebin) utility element. The `decodebin` element constructs a concrete set of decode operations based on the given input format and the decoder and demuxer elements available in the system. At a high level the decodebin abstracts the individual operations required to take encoded frames and produce raw video frames suitable for image transformation and inferencing.\n",
    "\n",
    "The next step in the pipeline after decoding is color space conversion which is handled by the [videoconvert](https://gstreamer.freedesktop.org/documentation/videoconvert/index.html?gi-language=c#videoconvert) element. The exact transformation required is specified by placing a [capsfilter](https://gstreamer.freedesktop.org/documentation/coreelements/capsfilter.html?gi-language=c#capsfilter) on the output of the videoconvert element. In this case we specify BGRx because this is the format used by the detection model.\n",
    "<a id='dl-streamer'></a>\n",
    "#### DL Streamer elements\n",
    "Elements that start with the prefix 'gva' are from DL Streamer and are provided as part of the OpenVINO™ toolkit. There are five DL Streamer elements used in this tutorial which we will describe here along with the properties that will be used. Refer to [DL Streamer elements page](https://github.com/opencv/gst-video-analytics/wiki/Elements) for the list of all DL Streamer elements and usages.  \n",
    "\n",
    "* [gvadetect](https://github.com/opencv/gst-video-analytics/wiki/gvadetect) - Runs detection with the OpenVINO™ inference engine. We will use it to detect vehicles in a frame and output their bounding boxes.\n",
    "\t- `model` - path to the inference model network file.\n",
    "\t- `device` - device to run inferencing on. \n",
    "\t- `inference-interval` - interval between inference requests, the bigger the value, the better the throughput. i.e. setting this property to 1 would mean run deteciton on every frame while setting it to 5 would run detection on every fifth frame.\n",
    "* [gvaclassify](https://github.com/opencv/gst-video-analytics/wiki/gvaclassify) - Runs classification with the OpenVINO™ inference engine. We will use it to label the bounding boxes that `gvadetect` output with the type and color of the vehicle. \n",
    "\t- `model` - path to the inference model network file.\n",
    "\t- `model-proc` - path to the model-proc file. More information on what a model-proc file is can be found in [section 2.4](#model-proc).\n",
    "\t- `device` - device to run inferencing on. \n",
    "    - `reclassify-interval` - How often to reclassify tracked objects. Only valid when used with `gvatrack`.\n",
    "* [gvawatermark](https://github.com/opencv/gst-video-analytics/wiki/gvawatermark) - Overlays detection and classification results on top of video data. We will do exeactly that. Parse the detected vehicle results metadata and create a video frame rendered with the bounding box aligned to the vehicle position; parse the classified vehicle result and label it on the bounding box.  \n",
    "* [gvafpscounter](https://github.com/opencv/gst-video-analytics/wiki/gvafpscounter) - Measure Frames Per Second across multiple streams and print to the output. \n",
    "\t- `starting-frame` specifies the frame to start collecting fps measurements. In this tutorial, we start at frame 10 to not include initialization time in our performance output.\n",
    "* [gvatrack](https://github.com/opencv/gst-video-analytics/wiki/gvatrack) - Identifies objects in frames where detection is skipped. This allows us to run object detection on fewer frames and increases overall throughput while still tracking the position and type of objects in every frame.\n",
    "\n",
    "### Properties\n",
    "Elements are configured using key, value pairs called properties. As an example the filesrc element has a property named `location` which specifies the file path for input.\n",
    "\n",
    "**Example**:\n",
    " ```filesrc location=cars_1900.mp4```.\n",
    "\n",
    "The documentation for each element (which can be viewed using the command line tool `gst-inspect-1.0`) describes its properties as well as the valid range of values for each property.\n",
    "\n",
    "<a id='setup'></a>\n",
    "## Setup\n",
    "### Import dependencies\n",
    "Import Python dependencies needed for displaying the results in this notebook  \n",
    "*Tip: select the cell and use **Shift+Enter** to run the cell*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qarpo.demoutils import *\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading OpenVINO™ models\n",
    "In this tutorial we use the `pedestrian-and-vehicle-detector-adas-0001` and the `vehicle-attributes-recognition-barrier-0039`models provided in the OpenVINO™ Open Model Zoo repository. We can download the OpenVINO™ IR model directly from the Open Model Zoo using the model downloader. To use models not provided in the repository (e.g. *mobilenet-ssd*) first download the model and use the [model optimizer](https://docs.openvinotoolkit.org/latest/_docs_MO_DG_Deep_Learning_Model_Optimizer_DevGuide.html) to convert it to the OpenVINO™ IR format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!downloader.py --name pedestrian-and-vehicle-detector-adas-0001 -o models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!downloader.py --name vehicle-attributes-recognition-barrier-0039 --precisions FP16 -o models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='model-proc'></a>\n",
    "### Preparing the model_proc file\n",
    "When performing inferencing, the model chosen will have a set of defined inputs and outputs. This information is shared with the inferencing elements in the form of a JSON file called a model_proc file. The elements can perform certain pre and post-processing operations to prepare the input video for inferencing and to make output more meaningful based on information in the model_proc. Model_proc files are not always required. They are used for configuration of your element but if the default values are sufficient it can be omitted.\n",
    "\n",
    "We will go over the basics here but you can go to [this page](https://github.com/opencv/gst-video-analytics/wiki/Model-preparation#2-model-pre--and-post-processing-specificaiton-file-) for more details on model_proc files and examples of .json files for various models from Open Model Zoo. \n",
    "\n",
    "#### Model_proc format\n",
    "The model_proc file is split into a few sections. \n",
    "First is the schema version. The schema can be found [here](https://github.com/opencv/gst-video-analytics/blob/master/gst/inference_elements/base/model_proc_schema.h). The version you are referencing when creating your model_proc should be reflected as `json_schema_version`.\n",
    "\n",
    "The input_preproc and output_postproc sections are both arrays of JSON objects, with each object representing a layer of the model. In version 1 of the schema, the input_preproc can be used to set the color format of the video coming in. \n",
    "\n",
    "In the output_postproc section, a layer's output can be converted to something meaningful to future elements in the pipeline, the application or the user viewing the data. \n",
    "One option for a converter is `tensor_to_bbox_ssd` which would be used with a single shot detector (SSD) model for things like object detection. This converter will create a GstVideoRegionOfInterestMeta metadata type, fill it with the bounding box of the detected object and confidence data from the model's output and post it on the GStreamer bus so it can be used by future elements.  \n",
    "Another converter is `tensor_to_label` which is used for classification models. It uses the `labels` property to put text labels on the output of the model. This is what we will use below in the model_proc file for our vehicle classification. The color layer of our model will tell us that the detected vehicle is part of class 1 but using our provided labels, `gvaclassify` will label the vehicle as \"white\".\n",
    "\n",
    "#### Default behavior\n",
    "Most commonly, a model will input video in BGR color format. If no model_proc is provided or if the input_preproc section is left empty it will default to this format.  \n",
    "Post processing is optional. The output will be GstGVATensorMeta if post processing is not performed. This will typically be the case if using the `gvainference` element. Post processing will be enabled if provided in the model_proc file **or** if the element can automatically detect the model output format. Currently the only automatically recognizable models for the inference elements is single shot detector models like object detection. The element will use the `tensor_to_bbox_ssd` converter to output bounding boxes of detected objects.\n",
    "\n",
    "We will not supply a model_proc file to our detection model. The input to the model should be BGR format (and we will do this conversion with another GStreamer element). It is an SSD based model so will output bounding boxes by default for the vehicles detected. We do not need to label the vehicles detected since we will be doing this with a classification model anyway. \n",
    "#### Make a directory to hold the model_proc file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p model_proc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### vehicle-attributes-recognition-barrier-0039\n",
    "Below we will generate a model_proc file for the classification model to configure our `gvaclassify` element. We skip the input_preproc section since our video will already be in the appropriate format for our model. We then provide two sections in the output_postproc for the two layers of the classiciation model `color` and `type`. In each layer's section we define that we want to use the `tensor_to_label` converter to label our output and the method `max` tells the element to choose the label with the highest confidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile model_proc/vehicle-attributes-recognition-barrier-0039.json\n",
    "{\n",
    "  \"json_schema_version\": \"1.0.0\",\n",
    "  \"input_preproc\": [],\n",
    "  \"output_postproc\": [\n",
    "    {\n",
    "      \"layer_name\": \"color\",\n",
    "      \"attribute_name\": \"color\",\n",
    "      \"labels\": [\n",
    "        \"white\",\n",
    "        \"gray\",\n",
    "        \"yellow\",\n",
    "        \"red\",\n",
    "        \"green\",\n",
    "        \"blue\",\n",
    "        \"black\"\n",
    "      ],\n",
    "      \"converter\": \"tensor_to_label\",\n",
    "      \"method\": \"max\"\n",
    "    },\n",
    "    {\n",
    "      \"layer_name\": \"type\",\n",
    "      \"attribute_name\": \"type\",\n",
    "      \"labels\": [\n",
    "        \"car\",\n",
    "        \"bus\",\n",
    "        \"truck\",\n",
    "        \"van\"\n",
    "      ],\n",
    "      \"converter\": \"tensor_to_label\",\n",
    "      \"method\": \"max\"\n",
    "    }\n",
    "  ]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The input video file\n",
    "We will use this input video when running our pipelines. Run the following cell to create a symlink and view the input video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ln -sf /data/reference-sample-data/object-detection-python/cars_1900.mp4 \n",
    "videoHTML('Input Video', ['cars_1900.mp4'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='vehicle-classification'></a>\n",
    "## Vehicle detection and classification\n",
    "### Section Overview\n",
    "In this section, you will learn how to:\n",
    "* Construct a media analytics pipeline using Gstreamer.\n",
    "* Submit the pipeline for execution on edge nodes.\n",
    "* Run the pipeline for vehicle detection and classification.\n",
    "* Watermark the inference results on the video.\n",
    "* Save the video file.\n",
    "* Play the video in Jupyter Notebook.\n",
    "\n",
    "### Pipeline\n",
    "The script in the next cell constructs a media analytics pipeline for vehicle detection and classification. For more information on GStreamer basics, see [Section 2 'About GStreamer'](#about-gstreamer) above or visit GStreamer's documentation pages [here](https://gstreamer.freedesktop.org/documentation/).\n",
    "\n",
    "The pipeline you will create will accept a video file input, decode it and run vehicle detection, followed by classification. It overlays the bounding boxes for detected vehicles and classification results on the video frame, downscales the video for better viewing in a browser and outputs it as an mp4 video file.\n",
    "```shell\n",
    "gst-launch-1.0 filesrc location=<input file> ! decodebin ! videoconvert ! \\\n",
    "gvadetect model=<model file> device=<device id> inference-interval=1 ! \\\n",
    "gvaclassify model=<model file 1> model-proc=<model_proc file 1> device=CPU ! \\\n",
    "gvawatermark ! videoconvert ! videoscale ! video/x-raw,width=960,height=540 ! vaapih264enc ! mp4mux ! filesink location=<output file>\n",
    "```\n",
    "The input video has 1080P(1920x1080) resolution and is downscaled to 960x540 at the output using the standard GStreamer element `videoscale`. This is done simply to reduce the buffering time for playback in a web browser and to make the bounding boxes and labels easier to see. You can remove the `videoscale ! video/x-raw,width=960,height=540` to output the high resolution video if you prefer.\n",
    "\n",
    "### Create job script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile vehicle_detection_and_classification.sh\n",
    "\n",
    "DEVICE=${2:-CPU}\n",
    "DETECT_MODEL=models/intel/pedestrian-and-vehicle-detector-adas-0001/FP16/pedestrian-and-vehicle-detector-adas-0001.xml\n",
    "CLASSIFY_MODEL=models/intel/vehicle-attributes-recognition-barrier-0039/FP16/vehicle-attributes-recognition-barrier-0039.xml\n",
    "CLASSIFY_MODEL_PROC=model_proc/vehicle-attributes-recognition-barrier-0039.json\n",
    "INPUT_FILE=${1:-cars_1900.mp4}\n",
    "INFERENCE_INTERVAL=1\n",
    "OUTPUT_FILE=resources/video/output/vehicle_detect_classify_output.mp4\n",
    "\n",
    "PIPELINE=\"filesrc location=$INPUT_FILE ! decodebin ! \\\n",
    "    videoconvert n-threads=4 ! capsfilter caps=\\\"video/x-raw,format=BGRx\\\" ! \\\n",
    "    gvadetect model=$DETECT_MODEL device=$DEVICE inference-interval=$INFERENCE_INTERVAL ! \\\n",
    "    gvaclassify model=$CLASSIFY_MODEL model-proc=$CLASSIFY_MODEL_PROC device=$DEVICE ! \\\n",
    "    gvawatermark ! videoconvert ! videoscale ! video/x-raw,width=960,height=540 ! vaapih264enc ! mp4mux ! filesink location=$OUTPUT_FILE\"\n",
    "echo \"gst-launch-1.0 $PIPELINE\"\n",
    "\n",
    "mkdir -p $(dirname \"${OUTPUT_FILE}\")\n",
    "rm -f \"$OUTPUT_FILE\"\n",
    "gst-launch-1.0 $PIPELINE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the pipeline\n",
    "The cell below, runs the pipeline and generates the video output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!./vehicle_detection_and_classification.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View the results\n",
    "After the job is completed, you can play the resulting mp4 video in the following cell. You will see the input video with the detections and classifications overlayed onto it. The detection is seen as a bounding box drawn around the vehicle. They will be labelled with text showing the classifications. In this case, type and color."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videoHTML('Vehicle Classification', \n",
    "          ['resources/video/output/vehicle_detect_classify_output.mp4'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='benchmarking'></a>\n",
    "## Pipeline Benchmarking\n",
    "### Section Overview\n",
    "In this section you will learn how to:\n",
    "* Construct/adjust the previous GStreamer pipeline to check performance. \n",
    "* Run pipelines demonstrating vehicle detection and vehicle tracking.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vehicle Detection\n",
    "#### Pipeline\n",
    "This pipeline is similar to the one we created above in [Section 3](#vehicle-classification) with a few key differences. In Section 3 we wanted to view the results of our inferencing. To do that we were watermarking the inferences onto the video, encoding it and saving it to an .mp4 file. Now we will remove all the video processing elements after `gvaclassify` and focus on the performance of inferencing.\n",
    "\n",
    "The pipeline will still accept a video file input, decode it and run vehicle detection, followed by classification. But instead of processing for video output, we will add the `gvafpscounter` element from DL Streamer to measure the FPS and then fakesink to not process the video any further.\n",
    "\n",
    "```shell\n",
    "gst-launch-1.0 filesrc location=<input file> ! decodebin ! videoconvert ! \\\n",
    "gvadetect model=<detection_model_file> device=<device_id> inference-interval=1 ! \\\n",
    "gvaclassify model=<classify_model_file> model-proc=<classify_model_proc> device=<device_id> ! \\\n",
    "gvafpscounter starting-frame=10 ! fakesink\n",
    "```\n",
    "\n",
    "There are three DL Streamer elements used. For a reminder of what they are used for see [Section 1.2.1](#dl-streamer).\n",
    "\n",
    "#### Create a job script for vehicle detection\n",
    "In the script below we achieve a stream density of four by repeating the pipeline command multiple times (designated by CHANNELS_COUNT) to start multiple pipeline instances.\n",
    "\n",
    "The script has two arguments, `$1` is the device type, CPU/GPU/HDDL; `$2` is the number of buffers that will be read from the input file. Lowering this number will reduce the overall running time but should not affect the average FPS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile vehicle_detection_benchmark.sh\n",
    "\n",
    "DEVICE=${1:-CPU}\n",
    "NUM_BUFFERS=${2:--1}\n",
    "\n",
    "DETECT_MODEL=models/intel/pedestrian-and-vehicle-detector-adas-0001/FP16/pedestrian-and-vehicle-detector-adas-0001.xml\n",
    "CLASSIFY_MODEL=models/intel/vehicle-attributes-recognition-barrier-0039/FP16/vehicle-attributes-recognition-barrier-0039.xml\n",
    "CLASSIFY_MODEL_PROC=model_proc/vehicle-attributes-recognition-barrier-0039.json\n",
    "INPUT_FILE=cars_1900.mp4\n",
    "INFERENCE_INTERVAL=1\n",
    "CHANNELS_COUNT=4\n",
    "PIPELINE=\"filesrc location=$INPUT_FILE num-buffers=$NUM_BUFFERS ! decodebin ! \\\n",
    "    videoconvert n-threads=4 ! capsfilter caps=\\\"video/x-raw,format=BGRx\\\" ! \\\n",
    "    gvadetect model=$DETECT_MODEL device=$DEVICE inference-interval=$INFERENCE_INTERVAL ! queue ! \\\n",
    "    gvaclassify model=$CLASSIFY_MODEL model-proc=$CLASSIFY_MODEL_PROC device=$DEVICE reclassify-interval=$INFERENCE_INTERVAL ! queue ! \\\n",
    "    gvafpscounter starting-frame=10 ! fakesink \"\n",
    "\n",
    "FINAL_PIPELINE_STR=\"\"\n",
    "for (( CURRENT_CHANNELS_COUNT=0; CURRENT_CHANNELS_COUNT < $CHANNELS_COUNT; ++CURRENT_CHANNELS_COUNT ))\n",
    "do\n",
    "  FINAL_PIPELINE_STR+=$PIPELINE\n",
    "done\n",
    "echo \"gst-launch-1.0 $FINAL_PIPELINE_STR\"\n",
    "gst-launch-1.0 $FINAL_PIPELINE_STR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the configured pipeline\n",
    "The cell below, runs the pipeline while printing out the performance information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!./vehicle_detection_benchmark.sh | tee vehicle_detection.log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vehicle Tracking\n",
    "#### Pipeline\n",
    "Here we are going to make a few adjustments to improve our performance. We set the `inference-interval` property on `gvadetect` to 10 meaning that detection will only be performed on every 10th frame. Similarly, we set the `reclassify-interval` on `gvaclassify` to 10. In addtion, we add the element `gvatrack` to identify vehicles in the frames where detection is skipped. **This allows us to run detection on fewer frames and increases overall throughput while still tracking the position and type of objects in every frame.**\n",
    "\n",
    "```shell\n",
    "gst-launch-1.0 filesrc location=<input file> ! decodebin ! videoconvert ! \\\n",
    "gvadetect model=<detection_model_file> device=<device_id> inference-interval=10 ! \\\n",
    "gvatrack ! gvaclassify model=<classify_model_file> model-proc=<classify_model_proc> device=<device_id> ! \\\n",
    "gvafpscounter starting-frame=10 ! fakesink\n",
    "```\n",
    "\n",
    "#### Create a script for vehicle tracking\n",
    "After constructing the vehicle tracking pipeline, the script also increases stream density by repeating the pipeline to start multiple pipeline instances like before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile vehicle_tracking_benchmark.sh\n",
    "\n",
    "DEVICE=${1:-CPU}\n",
    "NUM_BUFFERS=${2:--1}\n",
    "\n",
    "DETECT_MODEL=models/intel/pedestrian-and-vehicle-detector-adas-0001/FP16/pedestrian-and-vehicle-detector-adas-0001.xml\n",
    "CLASSIFY_MODEL=models/intel/vehicle-attributes-recognition-barrier-0039/FP16/vehicle-attributes-recognition-barrier-0039.xml\n",
    "CLASSIFY_MODEL_PROC=model_proc/vehicle-attributes-recognition-barrier-0039.json\n",
    "INPUT_FILE=cars_1900.mp4\n",
    "INFERENCE_INTERVAL=10\n",
    "CHANNELS_COUNT=4\n",
    "PIPELINE=\"filesrc location=$INPUT_FILE num-buffers=$NUM_BUFFERS ! decodebin ! \\\n",
    "    videoconvert n-threads=4 ! capsfilter caps=\"video/x-raw,format=BGRx\" ! \\\n",
    "    gvadetect model=$DETECT_MODEL device=$DEVICE inference-interval=$INFERENCE_INTERVAL ! queue ! \\\n",
    "    gvatrack ! queue ! gvaclassify model=$CLASSIFY_MODEL model-proc=$CLASSIFY_MODEL_PROC device=$DEVICE reclassify-interval=$INFERENCE_INTERVAL ! queue ! \\\n",
    "    gvafpscounter starting-frame=10 ! fakesink \"\n",
    "FINAL_PIPELINE_STR=\"\"\n",
    "for (( CURRENT_CHANNELS_COUNT=0; CURRENT_CHANNELS_COUNT < $CHANNELS_COUNT; ++CURRENT_CHANNELS_COUNT ))\n",
    "do\n",
    "  FINAL_PIPELINE_STR+=$PIPELINE\n",
    "done\n",
    "echo \"gst-launch-1.0 $FINAL_PIPELINE_STR\"\n",
    "gst-launch-1.0 $FINAL_PIPELINE_STR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the configured pipeline\n",
    "Prints the output of the running pipeline to the cell and logs the output to file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!./vehicle_tracking_benchmark.sh | tee vehicle_tracking.log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Comparison Chart\n",
    "For the jobs run above, we collect the performance for each job and plot a bar chart.\n",
    "#### Helper functions\n",
    "This function will parse the output file to find the average FPS. Since we ran four instances of the pipelines simultaneously, it adds the average of each individual pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_average_fps(file_name):\n",
    "    if not os.path.exists(file_name):\n",
    "        return None\n",
    "    with open(file_name, 'r') as fps_file:\n",
    "        for line in fps_file:\n",
    "            match_object=re.search(r'FPSCounter\\(average\\):\\ total=([0-9\\.]*).*',line)\n",
    "            if match_object:\n",
    "                return float(match_object.group(1))\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function will create the bar chart and table for 2 sets of average FPS numbers, one for the vehicle detection pipeline; one for the vehicle tracking pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_results(cols, rows, detection_fps, tracking_fps):\n",
    "    fig,ax = plt.subplots(figsize=(10,5))\n",
    "    cell_text=[]\n",
    "    cell_text.append(detection_fps)\n",
    "    cell_text.append(tracking_fps)\n",
    "    results_table = plt.table(cellText=cell_text, rowLabels=rows, colLabels=cols, cellLoc=\"center\")\n",
    "    results_table.scale(1,4)\n",
    "    results_table.auto_set_font_size(False)\n",
    "    results_table.set_fontsize(12)\n",
    "    x = np.arange(len(detection_fps))\n",
    "    width=0.4\n",
    "    det_bar = plt.bar(x - width/2, detection_fps, width=width, color=\"xkcd:blue\", label='Detection')\n",
    "    track_bar = plt.bar(x + width/2, tracking_fps, width=width, color=\"xkcd:azure\", label='Tracking')\n",
    "    plt.tick_params(axis='x', which='both', bottom=False, top=False, labelbottom=False)\n",
    "    plt.title('Vehicle Detection and Tracking')\n",
    "    plt.ylabel('Frames Per Second')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Displaying the Results\n",
    "The bar chart created below will show the average FPS numbers for all designated Intel® platforms; for each plaform, vehicle detection and vehicle tracking results will be drawn side-by-side to show the performance improvement.  \n",
    "\n",
    "**In general the vehicle tracking pipeline has better performance than the vehicle detection pipeline. This is due to the increase in the inference-interval. Since the vehicle is being tracked across frames we do not need to perform inference on every frame and we end up with higher throughput. Adjusting this value is critical to your use case to ensure that you are getting the best performance while still making sure to capture new objects that come into the frame.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arch_list = [('core_cpu', 'Intel Core\\ni5-6500TE\\nCPU'),\n",
    "             ('core_gpu', 'Intel Core\\ni5-6500TE\\nGPU'),\n",
    "             ('xeon', 'Intel Xeon\\nE3-1268L v5\\nCPU'),\n",
    "             ('hddlr', ' IEI Mustang\\nV100-MX8\\nVPU'),\n",
    "             ('up2', 'Intel Atom\\nx7-E3950\\nUP2/GPU')]\n",
    "\n",
    "detection_fps_results = []\n",
    "tracking_fps_results = []\n",
    "column_names = []\n",
    "row_names = ('Vehicle Detection\\nInference-interval=1','Vehicle Tracking\\nInference-interval=10')\n",
    "\n",
    "for arch, a_name in arch_list:\n",
    "    column_names.append(a_name)\n",
    "    if 'job_id_'+arch in vars():\n",
    "        fps = find_average_fps('{}.o{}'.format(vars()['job_name_'+arch], vars()['job_id_'+arch][0].split(\".\")[0]))\n",
    "        if fps:\n",
    "            detection_fps_results.append(fps)\n",
    "        else:\n",
    "            detection_fps_results.append(0)\n",
    "    else:\n",
    "        detection_fps_results.append(0)\n",
    "    if 'job_id_tracking_'+arch in vars():\n",
    "        fps = find_average_fps('{}.o{}'.format(vars()['job_name_tracking_'+arch], vars()['job_id_tracking_'+arch][0].split(\".\")[0]))\n",
    "        if fps:\n",
    "            tracking_fps_results.append(fps)\n",
    "        else:\n",
    "            tracking_fps_results.append(0)\n",
    "    else:\n",
    "        tracking_fps_results.append(0)\n",
    "        \n",
    "display_results(column_names, row_names, detection_fps_results, tracking_fps_results)\n",
    "display(widgets.HTML(value=defaultDisclaimer()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "276px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
