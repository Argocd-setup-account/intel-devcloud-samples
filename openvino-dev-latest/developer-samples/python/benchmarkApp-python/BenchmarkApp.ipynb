{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"top\"></a>\n",
    "# Benchmark Models on Intel® DevCloud for the Edge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intel® Distribution of OpenVINO™ Toolkit version check:\n",
    "You are currently using the latest development version of Intel® Distribution of OpenVINO™ Toolkit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "This notebook uses the [Intel® Distribution of OpenVINO™ Toolkit Benchmarking app](https://docs.openvinotoolkit.org/latest/_inference_engine_tools_benchmark_tool_README.html) to benchmark your on different hardware. Also, showcasing how you can optionally store your model files using AWS S3 service.\n",
    "\n",
    "Benchmark python tool provides estimation of deep learning inference performance on the supported devices. Performance can be measured for two inference modes: synchronous (latency-oriented) and asynchronous (throughput-oriented).  \n",
    "\n",
    "This tutorial benchmarks the deep learning model with \n",
    "1. Intel® Core™ CPU, Intel® Xeon® CPU, Intel® HD Graphics 530 GPU, Intel® Neural Compute Stick 2, Intel® Movidius™ Myriad™ X HDDL\n",
    "2. Workload distribution with [Multi plugin](https://docs.openvinotoolkit.org/latest/_docs_IE_DG_supported_plugins_MULTI.html)  \n",
    "\n",
    "#### Running this notebook without  AWS S3 bucket\n",
    "If you are not using AWS S3 service to store model files, please read the instructions below before running this notebook.\n",
    "1. Upload model files (IR files (.xml+.bin)) to your Intel® DevCloud instance inside the same directory as that of jupyter notebook. <br>Folder structure should be same format as below:\n",
    "    <br> \n",
    "    \n",
    "    *[Model_Name]/IR_models/[FP16 or FP32]/[IR FILE NAME].xml* \n",
    "    \n",
    "    \n",
    "2. Initialize \"models\" list with name of models you have uploaded for benchmarking. [See section 1.9 - Creating list of FP16 and FP32 models](#Creating-lists-of-FP16-and-FP32-models)<br>\n",
    "    Example: *models = ['MobileNetV2-keras', 'NASNetMobile-keras']*<br>\n",
    "\n",
    "\n",
    "3. Skip **section 1.4 - Enter AWS S3 bucket name** and **section 1.8 - Downloading Model from S3**\n",
    "\n",
    "### How It Works\n",
    "Upon start-up, the application reads command-line parameters and loads a network and images/binary files to the Inference Engine plugin, which is chosen depending on a specified device. The number of infer requests and execution approach depend on the mode defined with the -api and -niter command-line parameter. \n",
    "\n",
    "In this tutorial, we use following input parameters with the benchmark app:\n",
    "\n",
    "- -m: deep learning model to infer in Intermediate Representation (.xml & .bin), e.g. Resnet-50 tensorflow model. \n",
    "- -d: device to offload inference workload\n",
    "- -niter: number of iterations. \n",
    "- -api: sync or async\n",
    "- --report_type: information about details counter e.g. FPS and latency\n",
    "- --report_folder: Path to a folder where statistics report is stored.\n",
    "- -i: input image/video, If a topology is not data sensitive, you can skip the input parameter. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enter AWS IAM user credentials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provide AWS IAM user credentials (AccessKeyId,SecretAccessKey) to configure AWS SDK. \n",
    "1. Sign in to [IAM Console](https://console.aws.amazon.com/iam/home#/home)\n",
    "2. Create a new IAM User with programmatic access \n",
    "3. Attach 'AmazonS3FullAccess' & 'AmazonSESFullAccess' policy to IAM User\n",
    "4. Create Access Key - Please download 'Download.csv' file for future reference. AWS Access keys are used to configure AWS CLI.  \n",
    "\n",
    "\n",
    "<div class=note><i><b>Note: </b>For detailed instructions, Please follow this <a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_users_create.html#id_users_create_console\">link</a></i></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide AWS IAM user credentials (AccessKeyId,SecretAccessKey)\n",
    "ACCESS_KEY_ID = \"AWS AccessKeyId\"\n",
    "SECRET_ACCESS_KEY = \"AWS SecretAccessKey\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enter AWS S3 bucket name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provide the AWS S3 bucket name that contains OpenVINO optimized model files (IR Files) in zip format. \n",
    "\n",
    "**Format**: [Model_Name].zip   \n",
    "**Folder structure after unzip**: [Model_Name]/IR_models/[FP16 or FP32]/[IR FILE NAME].xml  \n",
    "**Example**: MobileNetV2-keras.zip  \n",
    "**Folder structure example**: MobileNetV2-keras/IR_models/FP16/MobileNetV2-keras.xml<br><br>\n",
    "<div class=note><i><b>Note: </b>Do not place any other files or folders in this bucket</i></div><br>\n",
    "<div class=tip><b>Tip: </b>Skip this section if you are not using AWS S3 bucket to store model files</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET_NAME = \"S3 Bucket Name\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enter Email address to send notification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provide AWS SES verfied email address of sender and recipient. You can also provide same email address for both sender and reciever.  \n",
    "For Email verification procedure, refer [here](https://docs.aws.amazon.com/ses/latest/DeveloperGuide/verify-email-addresses-procedure.html).<br><br>\n",
    "<div class=tip><b>Tip: </b>Update AWS_REGION with the AWS region where you have verfied email addreses.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace sender@example.com with your \"From\" address.\n",
    "# This address must be verified with Amazon SES.\n",
    "SENDER = \"Email Address of Sender\"\n",
    "\n",
    "# Replace recipient@example.com with a \"To\" address. If your account \n",
    "# is still in the sandbox, this address must be verified.\n",
    "RECIPIENT = \"Email Address of Recipient\"\n",
    "\n",
    "# Replace us-west-2 with the AWS Region you're using for Amazon SES.\n",
    "AWS_REGION = \"us-west-2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Install boto3 & progress package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip3 install --upgrade pip\n",
    "!pip3 install -r requirements.txt "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "import os\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div class=tip><b>Troubleshooting Tip: </b> If you have faced any issues in importing packages, please restart the kernel and run the jupyter notebook again</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading Model from S3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download all the files from the S3 bucket to the Intel® DevCloud instance<br><br>\n",
    "<div class=tip><b>Tip: </b>Skip this section if you are not using AWS S3 bucket to store model files</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "from zipfile import ZipFile\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "# initializing list. \n",
    "models=[]\n",
    "\n",
    "try:\n",
    "    # initiate s3 resource\n",
    "    s3 = boto3.resource('s3',aws_access_key_id=ACCESS_KEY_ID, aws_secret_access_key=SECRET_ACCESS_KEY)\n",
    "    \n",
    "    # select bucket\n",
    "    my_bucket = s3.Bucket(BUCKET_NAME)\n",
    "\n",
    "    # download file into current directory\n",
    "    for s3_object in my_bucket.objects.all():\n",
    "        # Need to split s3_object.key into path and file name, else it will give error file not found.\n",
    "        path, file = os.path.split(s3_object.key)\n",
    "        if(file):\n",
    "            my_bucket.download_file(Key=s3_object.key,Filename=file)\n",
    "            models.append(file.split(\".\")[0])\n",
    "            zip_file = ZipFile(file)\n",
    "            zip_file.extractall()\n",
    "            \n",
    "except ClientError as e:\n",
    "    print(e.response['Error']['Message'])\n",
    "\n",
    "if len(models) == 0:\n",
    "    print(\"S3 bucket is empty. Please uplaod files to S3 bucket\")\n",
    "else: \n",
    "    print(\"List of available models:\")\n",
    "    print(models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating lists of FP16 and FP32 models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=tip><b>Tip: </b>If you want to manually provide list of models, initialize \"models\" list with name of models you wish to benchmark.<br>\n",
    "For example: models = ['MobileNetV2-keras', 'NASNetMobile-keras']</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment below line if you want to manually provide list of models. For example: models = ['MobileNetV2-keras', 'NASNetMobile-keras']\n",
    "# models = [list of model separated by commas] \n",
    "\n",
    "models_fp32 = []\n",
    "models_fp16 = []\n",
    "for model in models:\n",
    "    if 'FP32' in os.listdir(model+'/IR_models'):\n",
    "        models_fp32.append(model)\n",
    "    elif 'FP16' in os.listdir(model+'/IR_models'): \n",
    "        models_fp16.append(model)\n",
    "print(\"FP32 Models:\",models_fp32)\n",
    "print(\"FP16 Models:\",models_fp16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating job file\n",
    "Till now, we ran all the above steps on a single development node with one core allocated for your account. Now we want to run inference on different edge compute nodes on the Intel® DevCloud for the Edge to benchmark the inference performance. For that, we will submit the inference jobs for each edge device in a queue. For each job, we will specify the type of the edge compute node that must be allocated for the job.\n",
    "\n",
    "The job file in the below cell is written in Bash, and will be executed directly on the edge compute node. Run the following cell to write this in to the file \"benchmark_app_job.sh\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=note><i><b>Note: </b>Default model path is [Model Name]/IR_models/[FP16 or FP32]/[IR FILE NAME].xml.</i></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile benchmark_app_job.sh\n",
    "\n",
    "# The default path for the job is your home directory, so we change directory to where the files are.\n",
    "cd $PBS_O_WORKDIR\n",
    "JOB_ID=`basename ${0} | cut -f1 -d\".\"`\n",
    "OUTPUT_FILE=$1\n",
    "DEVICE=$2\n",
    "#FP_MODEL=$3\n",
    "API=$3\n",
    "\n",
    "# Provide the model path\n",
    "# Argument 4 will be model name\n",
    "# Update the .xml path(relative path) if required \n",
    "\n",
    "MODEL_DIR=$4/IR_models/$5\n",
    "\n",
    "MODEL_FILE=$(ls $PBS_O_WORKDIR/${MODEL_DIR} | grep .xml)\n",
    "MODEL_PATH=${MODEL_DIR}/${MODEL_FILE}\n",
    "\n",
    "# Benchmark Application script writes output to a file inside a directory. We make sure that this directory exists.\n",
    "#  The output directory is the first argument of the bash script\n",
    "mkdir -p $OUTPUT_FILE\n",
    "\n",
    "SAMPLEPATH=$PBS_O_WORKDIR\n",
    "\n",
    "rm -f ${OUTPUT_FILE}/*\n",
    "\n",
    "echo ${SAMPLEPATH}/${OUTPUT_FILE} > benchmark_filename_${JOB_ID}.txt\n",
    "\n",
    "# Running the benchmark application code\n",
    "\n",
    "python3 /opt/intel/openvino/deployment_tools/tools/benchmark_tool/benchmark_app.py -m ${SAMPLEPATH}/${MODEL_PATH} \\\n",
    "            -d $DEVICE \\\n",
    "            -niter 10 \\\n",
    "            -api $API \\\n",
    "            --report_type detailed_counters \\\n",
    "            --report_folder ${SAMPLEPATH}/${OUTPUT_FILE}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the job script, we can submit the jobs to edge compute nodes. In the IoT DevCloud, you can do this using the qsub command. We can submit object_detection_job to 5 different types of edge compute nodes simultaneously or just one node at at time.\n",
    "\n",
    "There are three options of qsub command that we use for this:\n",
    "\n",
    "-l : this option lets us select the number and the type of nodes using nodes={node_count}:{property}.\n",
    "\n",
    "-F : this option lets us send arguments to the bash script.\n",
    "\n",
    "-N : this option lets use name the job so that it is easier to distinguish between them.\n",
    "If you are curious to see the available types of nodes on the IoT DevCloud, run the following optional cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "node_list = []\n",
    "nodes = subprocess.check_output(\"pbsnodes | grep compnode | sort | uniq | awk ' /'properties'/  {print $3}'\",shell = True)\n",
    "nodes = nodes.decode('utf-8')\n",
    "nodes = nodes.split('\\n')\n",
    "for node in nodes:\n",
    "    node_list.append(node.split(',')[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wait until the benchmarking report files are written \n",
    "\n",
    "We submit the job to different hardware platforms using a job queue. We will have to wait until we get the results back from our specified hardware. In the following script, we check if the reports file is generated that shows the job is complete. Until, the job is completed, we print dots on the screen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wait_for_job_to_finish(job_id,verbose,model):\n",
    "    # time-out set to 10 mins\n",
    "    time_out = 10\n",
    "    start_time = time.time()\n",
    "    # print(job_id[0]) \n",
    "    if job_id:\n",
    "        \n",
    "        print(\"Job submitted to the queue. Waiting for it to complete .\", end=\"\")\n",
    "        filename = \"benchmark_filename_{}.txt\".format(job_id[0].split(\".\")[0])\n",
    "        \n",
    "        while not os.path.exists(filename):  # Wait until the file report is created.\n",
    "            time.sleep(1)\n",
    "            print(\".\", end=\"\")\n",
    "\n",
    "        # Print the results\n",
    "        with open(filename) as f:\n",
    "            results_dir = f.read().split(\"\\n\")[0]\n",
    "            \n",
    "        report_filename = os.path.join(results_dir, \"benchmark_report.csv\") # Wait until the file report is created.\n",
    "        while not os.path.exists(report_filename):\n",
    "            time.sleep(1)\n",
    "            print(\".\", end=\"\")\n",
    "            current_time = time.time()\n",
    "            seconds_elapsed = current_time - start_time\n",
    "            minutes = seconds_elapsed / 60.0 \n",
    "            if(minutes >= time_out):\n",
    "                print(\" Time out: Exceeded maximum time of 10 mins !!!\")\n",
    "                return {\n",
    "                        \"Throughput (FPS)\": 0.0, \n",
    "                        \"Load network time (ms)\" : 0.0,\n",
    "                        \"Read network time (ms)\" : 0.0,\n",
    "                        \"First inference time (ms)\": 0.0,\n",
    "                        \"Total execution time (ms)\": 0.0,\n",
    "                        \"Total number of iterations\": 0.0}\n",
    "        \n",
    "        df = pd.read_csv(report_filename, delimiter=\";\")\n",
    "        \n",
    "        if(verbose):\n",
    "            print(df)\n",
    "        \n",
    "        throughput = float(df.loc[\"throughput\"][0])\n",
    "        device = df.loc[\"target device\"][0]\n",
    "        load_time = float(df.loc[\"load network time (ms)\"][0])\n",
    "        read_time = float(df.loc[\"read network time (ms)\"][0])\n",
    "        infer_time = float(df.loc[\"first inference time (ms)\"][0])\n",
    "        exec_time = float(df.loc[\"total execution time (ms)\"][0])\n",
    "        iterations = int(df.loc[\"total number of iterations\"][0])\n",
    "        \n",
    "        os.remove(filename) # Cleanup\n",
    "        print(\"\\n===Benchmarking of {} model completed===\\n\".format(model))\n",
    "        \n",
    "    else:\n",
    "        print(\"Error in job submission.\")\n",
    "        \n",
    "        throughput = None\n",
    "        device = None\n",
    "        load_time = None\n",
    "        read_time = None\n",
    "        \n",
    "    return {\n",
    "            \"Throughput (FPS)\": throughput, \n",
    "            \"Load network time (ms)\" : load_time,\n",
    "            \"Read network time (ms)\" : read_time,\n",
    "            \"First inference time (ms)\": infer_time,\n",
    "            \"Total execution time (ms)\": exec_time,\n",
    "            \"Total number of iterations\": iterations}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above wait_for_job_to_finish() function returns throughput, load network time and read network time. We save these return values in a dictionary called benchmarks to be used later in graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize output dictionary \n",
    "output = {} # Save benchmark reports of all model into a dict\n",
    "for model in models:\n",
    "    output[model]={}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark Individual system with the deep learning model\n",
    "The Multi-Device plugin automatically assigns inference requests to available computational devices to execute the requests in parallel. \n",
    "\n",
    "This example shows how to use MULTI plugin from the Intel® Distribution of OpenVINO™ toolkit.\n",
    "First, let's take a look at the model's inference performance on each single device."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=note><i><b>Note: </b> Set verbose flag to 'True' for printing intermediate benchmark outputs from each node</i></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the Benchmark tool app with Intel® Core™ CPU\n",
    "In the cell below, we submit a job to an <a \n",
    "    href=\"https://software.intel.com/en-us/iot/hardware/iei-tank-dev-kit-core\">IEI \n",
    "    Tank* 870</a> edge node with an <a \n",
    "    href=\"https://devcloud.intel.com/edge/devices/intel-core-i5-6500te-cpu/\">Intel® Core™ i5 6500TE CPU</a>. The inference workload will run on the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "node = 'idc001skl'\n",
    "if node in node_list:\n",
    "    print(\"Submitting a job to an edge compute node with an Intel Core CPU...\")\n",
    "    for model in models_fp16:\n",
    "        benchmarks = {}\n",
    "        try:\n",
    "            # Submit job to the queue\n",
    "            job_id_core = !qsub benchmark_app_job.sh -l nodes=1:{node} -F \"results/core/{model} CPU async {model} FP16\"\n",
    "            benchmarks['Intel® Core™ CPU'] = wait_for_job_to_finish(job_id_core,verbose,model)\n",
    "            output[model].update(benchmarks)\n",
    "        except Exception as e:\n",
    "            print(\"\\n===Failed to perform benchmarking for {} model !===\\n\".format(model))\n",
    "            print(\"Error: \", e)\n",
    "\n",
    "    for model in models_fp32:\n",
    "        benchmarks = {}\n",
    "        try:\n",
    "            # Submit job to the queue\n",
    "            job_id_core = !qsub benchmark_app_job.sh -l nodes=1:{node} -F \"results/core/{model} CPU async {model} FP32\"\n",
    "            benchmarks['Intel® Core™ CPU'] = wait_for_job_to_finish(job_id_core,verbose,model)\n",
    "            output[model].update(benchmarks)\n",
    "        except Exception as e:\n",
    "            print(\"\\n===Failed to perform benchmarking for {} model !===\\n\".format(model))\n",
    "            print(\"Error: \", e)\n",
    "else:\n",
    "    print(\" {} not currently available. Try changing node !!! \".format(node))\n",
    "    print(\"List of available nodes: {}\".format(node_list))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the Benchmark tool app with Intel® Xeon® CPU\n",
    "In the cell below, we submit a job to an <a \n",
    "    href=\"https://software.intel.com/en-us/iot/hardware/iei-tank-dev-kit-core\">IEI \n",
    "    Tank* 870-Q170</a> edge node with an <a \n",
    "    href=\"https://devcloud.intel.com/edge/devices/intel-xeon-e3-1268l-v5-cpu/\">Intel® \n",
    "    Xeon® Processor E3-1268L v5</a>. The inference workload will run on the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "node = 'idc007xv5'\n",
    "if node in node_list:\n",
    "    print(\"Submitting a job to an edge compute node with an Intel Xeon CPU...\")\n",
    "    for model in models_fp16:\n",
    "        benchmarks={}\n",
    "        try:\n",
    "            # Submit job to the queue\n",
    "            job_id_xeon = !qsub benchmark_app_job.sh -l nodes=1:{node} -F \"results/xeon/{model} CPU async {model} FP16\"      \n",
    "            benchmarks[\"Intel® Xeon® CPU\"] = wait_for_job_to_finish(job_id_xeon,verbose,model)\n",
    "            output[model].update(benchmarks)\n",
    "        except Exception as e:\n",
    "            print(\"\\n===Failed to perform benchmarking for {} model !===\\n\".format(model))\n",
    "            print(\"Error: \", e)\n",
    "\n",
    "    for model in models_fp32:\n",
    "        benchmarks={}\n",
    "        try:\n",
    "            # Submit job to the queue\n",
    "            job_id_xeon = !qsub benchmark_app_job.sh -l nodes=1:{node} -F \"results/xeon/{model} CPU async {model} FP32\"      \n",
    "            benchmarks[\"Intel® Xeon® CPU\"] = wait_for_job_to_finish(job_id_xeon,verbose,model)\n",
    "            output[model].update(benchmarks)\n",
    "        except Exception as e:\n",
    "            print(\"\\n===Failed to perform benchmarking for {} model !===\\n\".format(model))\n",
    "            print(\"Error: \", e)\n",
    "else:\n",
    "    print(\" {} not currently available. Try changing node !!! \".format(node))\n",
    "    print(\"List of available nodes: {}\".format(node_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run Benchmark tool application with Intel® HD Graphics 530 GPU\n",
    "In the cell below, we submit a job to an <a \n",
    "    href=\"https://software.intel.com/en-us/iot/hardware/iei-tank-dev-kit-core\">IEI \n",
    "    Tank* 870</a> edge node with an <a \n",
    "    href=\"https://devcloud.intel.com/edge/devices/intel-hd-530-gpu/\">Intel® HD Graphics 530 GPU</a>. The inference workload will run on the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "node = 'idc001skl'\n",
    "if node in node_list:\n",
    "    print(\"Submitting a job to an edge compute node with an Intel Core CPU and an Intel GPU...\")\n",
    "    for model in models_fp16:\n",
    "        benchmarks={}\n",
    "        try: \n",
    "            # Submit job to the queue\n",
    "            job_id_gpu = !qsub benchmark_app_job.sh -l nodes=1:{node} -F \"results/gpu/{model} GPU async {model} FP16\"        \n",
    "            benchmarks[\"Intel® HD Graphics 530 GPU\"] = wait_for_job_to_finish(job_id_gpu,verbose,model)\n",
    "            output[model].update(benchmarks)\n",
    "        except Exception as e:\n",
    "            print(\"\\n===Failed to perform benchmarking for {} model !===\\n\".format(model))\n",
    "            print(\"Error: \", e)\n",
    "\n",
    "    for model in models_fp32:\n",
    "        benchmarks={}\n",
    "        try: \n",
    "            # Submit job to the queue\n",
    "            job_id_gpu = !qsub benchmark_app_job.sh -l nodes=1:{node} -F \"results/gpu/{model} GPU async {model} FP32\"        \n",
    "            benchmarks[\"Intel® HD Graphics 530 GPU\"] = wait_for_job_to_finish(job_id_gpu,verbose,model)\n",
    "            output[model].update(benchmarks)\n",
    "        except Exception as e:\n",
    "            print(\"\\n===Failed to perform benchmarking for {} model !===\\n\".format(model))\n",
    "            print(\"Error: \", e)\n",
    "\n",
    "else:\n",
    "    print(\" {} not currently available. Try changing node !!! \".format(node))\n",
    "    print(\"List of available nodes: {}\".format(node_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run Benchmark tool application with Intel® Neural Compute Stick 2\n",
    "In the cell below, we submit a job to an <a \n",
    "    href=\"https://software.intel.com/en-us/iot/hardware/iei-tank-dev-kit-core\">IEI \n",
    "    Tank* 870</a> edge node with an <a \n",
    "    href=\"https://devcloud.intel.com/edge/devices/intel-movidius-myriadx-vpu-1/\">Intel® Neural Compute Stick 2 VPU</a>. The inference workload will run on the NCS2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "node = 'idc004nc2'\n",
    "if node in node_list:\n",
    "    if len(models_fp16) != 0:\n",
    "        print(\"Submitting job to an edge compute node with Intel NCS2...\")\n",
    "        for model in models_fp16:\n",
    "            benchmarks={}\n",
    "            try:\n",
    "                # Submit job to the queue\n",
    "                job_id_ncs2 = !qsub benchmark_app_job.sh -l nodes=1:{node} -F \"results/ncs2/{model} MYRIAD async {model} FP16\"    \n",
    "                benchmarks[\"Intel® Neural Compute Stick 2\"] = wait_for_job_to_finish(job_id_ncs2,verbose,model)\n",
    "                output[model].update(benchmarks)\n",
    "            except Exception as e:\n",
    "                print(\"\\n===Failed to perform benchmarking for {} model !===\\n\".format(model))\n",
    "                print(\"Error: \", e)\n",
    "    else:\n",
    "        print(\"FP32 models are incompatible with Intel NCS2...\")\n",
    "else:\n",
    "    print(\" {} not currently available. Try changing node !!! \".format(node))\n",
    "    print(\"List of available nodes: {}\".format(node_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run Benchmark tool application with Intel® Movidius™ Myriad™ X HDDL\n",
    "In the cell below, we submit a job to an <a \n",
    "    href=\"https://software.intel.com/en-us/iot/hardware/iei-tank-dev-kit-core\">IEI \n",
    "    Tank* 870</a> edge node with an <a \n",
    "    href=\"https://devcloud.intel.com/edge/devices/intel-movidius-myriadx-vpu-8/\">Intel® Movidius™ Myriad™ X HDDL VPU</a>. The inference workload will run on the HDDL-R."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node = 'idc002mx8'\n",
    "if node in node_list:\n",
    "    if len(models_fp16) != 0:\n",
    "        print(\"Submitting a job to an edge compute node with Intel Movidius HDDL-R...\")\n",
    "        for model in models_fp16:\n",
    "            benchmarks={}\n",
    "            try:\n",
    "                # Submit job to the queue\n",
    "                job_id_hddlr = !qsub benchmark_app_job.sh -l nodes=1:{node} -F \"results/hddlr/{model} HDDL async {model} FP16\" \n",
    "                benchmarks[\"Intel® Movidius™ Myriad™ X HDDL\"] = wait_for_job_to_finish(job_id_hddlr,verbose,model)\n",
    "                output[model].update(benchmarks)\n",
    "            except Exception as e:\n",
    "                print(\"\\n===Failed to perform benchmarking for {} model !===\\n\".format(model))\n",
    "                print(\"Error: \", e)\n",
    "    else:\n",
    "        print(\"FP32 models are incompatible with Intel HDDL-R...\")\n",
    "else:\n",
    "    print(\" {} not currently available. Try changing node !!! \".format(node))\n",
    "    print(\"List of available nodes: {}\".format(node_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi plugin\n",
    "Now let's try [MULTI plugin](https://docs.openvinotoolkit.org/latest/_docs_IE_DG_supported_plugins_MULTI.html) with different combination of available Inference Engine devices.\n",
    "\n",
    "Multi-Device plugin automatically assigns inference requests to available computational devices to execute the requests in parallel. Potential gains are as follows:\n",
    "\n",
    "- Improved throughput that multiple devices can deliver (compared to single-device execution)\n",
    "- More consistent performance, since the devices can now share the inference burden (so that if one device is becoming too busy, another device can take more of the load)\n",
    "\n",
    "Notice that we use multi-device and the application logic doesn't have to be changed. Meaning you don't need to explicitly load the network to every device, create and balance the inference requests and so on.\n",
    "\n",
    "####  Run Benchmark tool application with MULTI: Intel® Core™ CPU, Intel® HD Graphics 530 GPU\n",
    "In the cell below, we submit a job to an <a \n",
    "    href=\"https://software.intel.com/en-us/iot/hardware/iei-tank-dev-kit-core\">IEI \n",
    "    Tank* 870</a> edge node with an <a \n",
    "    href=\"https://devcloud.intel.com/edge/devices/intel-core-i5-6500te-cpu/\">Intel® Core™ i5 6500TE CPU</a> and <a \n",
    "    href=\"https://devcloud.intel.com/edge/devices/intel-hd-530-gpu/\">Intel® HD Graphics 530 GPU</a>. The inference workload will run on the MULTI:CPU,GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node = 'idc001skl'\n",
    "if node in node_list:\n",
    "    if len(models_fp16) != 0:\n",
    "        print(\"Submitting a job to an edge compute node with an CPU and GPU...\")\n",
    "        for model in models_fp16:\n",
    "            benchmarks={}\n",
    "            try:\n",
    "                # Submit job to the queue\n",
    "                job_id_cpu_gpu = !qsub benchmark_app_job.sh -l nodes=1:{node} -F \"results/cpu_gpu/{model} MULTI:CPU,GPU sync {model} FP16\" \n",
    "                benchmarks[\"MULTI:Intel® Core™ CPU,Intel® HD Graphics 530 GPU\"] = wait_for_job_to_finish(job_id_cpu_gpu,verbose,model)\n",
    "                output[model].update(benchmarks)\n",
    "            except Exception as e:\n",
    "                print(\"\\n===Failed to perform benchmarking for {} model !===\\n\".format(model))\n",
    "                print(\"Error: \", e)\n",
    "    else:\n",
    "        print(\"FP32 models are incompatible with Multi plugins...\")\n",
    "else:\n",
    "    print(\" {} not currently available. Try changing node !!! \".format(node)) \n",
    "    print(\"List of available nodes: {}\".format(node_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run Benchmark tool application with MULTI:Intel® Movidius™ Myriad™ X HDDL,Intel® Core™ CPU\n",
    "In the cell below, we submit a job to an <a \n",
    "    href=\"https://software.intel.com/en-us/iot/hardware/iei-tank-dev-kit-core\">IEI \n",
    "    Tank* 870</a> edge node with an <a \n",
    "    href=\"https://devcloud.intel.com/edge/devices/intel-core-i5-6500te-cpu/\">Intel® Core™ i5 6500TE CPU</a> and <a \n",
    "    href=\"https://devcloud.intel.com/edge/devices/intel-movidius-myriadx-vpu-8/\">Intel® Movidius™ Myriad™ X HDDL VPU</a>. The inference workload will run on the MULTI:HDDL,CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node = 'idc002mx8'\n",
    "if node in node_list:\n",
    "    if len(models_fp16) != 0:\n",
    "        print(\"Submitting a job to an edge compute node with Intel CPU and Intel Movidius HDDL-R...\")\n",
    "        for model in models_fp16:\n",
    "            benchmarks={}\n",
    "            try:\n",
    "                # Submit job to the queue\n",
    "                job_id_cpu_hddl = !qsub benchmark_app_job.sh -l nodes=1:{node} -F \"results/cpu_hddl/{model} MULTI:HDDL,CPU async {model} FP16\" \n",
    "                benchmarks[\"MULTI:Intel® Movidius™ Myriad™ X HDDL,Intel® Core™ CPU\"] = wait_for_job_to_finish(job_id_cpu_hddl,verbose,model)\n",
    "                output[model].update(benchmarks)\n",
    "            except Exception as e:\n",
    "                print(\"\\n===Failed to perform benchmarking for {} model !===\\n\".format(model))\n",
    "                print(\"Error: \", e)\n",
    "    else:\n",
    "        print(\"FP32 models are incompatible with Multi plugins...\")\n",
    "else:\n",
    "    print(\" {} not currently available. Try changing node !!! \".format(node))  \n",
    "    print(\"List of available nodes: {}\".format(node_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark Report\n",
    "\n",
    "The running time of each inference task is recorded in output dictionary. Run the cell below to plot the results of all jobs side-by-side.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Function to Plot Bar Chart\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patheffects as PathEffects\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "def plot_benchmarks(model,metric):\n",
    "    \n",
    "    latency = {}\n",
    "    no_number = False\n",
    "    for device in output[model]:\n",
    "        if isinstance(output[model][device][metric], str):\n",
    "            no_number = True\n",
    "        else:\n",
    "            latency[device] = output[model][device][metric]    \n",
    "        \n",
    "    if not no_number:\n",
    "        plt.figure(figsize=(18,8))\n",
    "        plt.bar(*zip(*latency.items()));\n",
    "        plt.xticks(fontsize=14);\n",
    "        plt.yticks(fontsize=18);\n",
    "        plt.ylabel(metric, fontsize=20);\n",
    "\n",
    "        rects = plt.gca().patches\n",
    "\n",
    "        # Make some labels.\n",
    "        labels = [\"{:,.2f}\".format(i) for i in latency.values()]\n",
    "\n",
    "        for rect, label in zip(rects, labels):\n",
    "            height = rect.get_height()\n",
    "            plt.gca().text(rect.get_x() + rect.get_width() / 2, height/2.0, label,\n",
    "                    ha=\"center\", va=\"bottom\", fontsize=20, color=\"white\", path_effects=[PathEffects.withStroke(linewidth=2, foreground=\"black\")])\n",
    "            \n",
    "    else:\n",
    "        print(\"ERROR: Field '{}' has text strings. Can't plot it.\".format(metric))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "key = lambda out:out[next(iter(out))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing Benchmark Results Tabular Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "import json\n",
    "\n",
    "devices_list = ['Intel® Neural Compute Stick 2','Intel® Core™ CPU','Intel® HD Graphics 530 GPU','MULTI:Intel® Core™ CPU,Intel® HD Graphics 530 GPU','Intel® Movidius™ Myriad™ X HDDL','MULTI:Intel® Movidius™ Myriad™ X HDDL,Intel® Core™ CPU','Intel® Xeon® CPU']\n",
    "\n",
    "link_file = open('device_link.json')\n",
    "data = json.load(link_file)\n",
    "df ={}\n",
    "\n",
    "for model in output:\n",
    "    print(\"\\n {} \\n\".format(model))\n",
    "    throughput = 0.0\n",
    "    for device in output[model]:\n",
    "        if (output[model][device]['Throughput (FPS)'] > throughput):\n",
    "            throughput= output[model][device]['Throughput (FPS)']\n",
    "            best_device = device\n",
    "            link = data[device]\n",
    "    display(HTML('<b>Model Name</b>: {}'.format(model)))\n",
    "    display(HTML('<b>Best Device</b>(Based on Throughput) : {}'.format(best_device)))\n",
    "    display(HTML('<b>Buy Now</b>: <a href=\" {}\">{}</a>'.format(link,link)))\n",
    "    display(HTML('<u>Benchmark Results</u>'))\n",
    "    df[model] = pd.DataFrame.from_dict(output[model], orient='index')\n",
    "    if model in models_fp16:\n",
    "        df[model]['Model Precision'] = 'FP16'\n",
    "    elif model in models_fp32:\n",
    "        df[model]['Model Precision'] = 'FP32'\n",
    "    df[model] = df[model].reindex(devices_list).dropna(axis=0)\n",
    "    display(HTML(df[model].to_html()))\n",
    "link_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ploting Bar chart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose Model name and metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    interact(plot_benchmarks, model=output.keys(),metric=key(output[next(iter(output))]).keys());\n",
    "except :\n",
    "    print(\"An error occured !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Email Notification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Notebook URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "user_info = subprocess.check_output(\"pwd\")\n",
    "user_info = user_info[1:-1]\n",
    "user_info = user_info.decode('utf-8')\n",
    "user_info = user_info.split('/')\n",
    "user = user_info[1]\n",
    "path = \"\"\n",
    "for i in range(2,len(user_info)):\n",
    "    path = path + \"/\" + user_info[i]\n",
    "notebook_url = \"https://jupyter.edge.devcloud.intel.com/user/{}/notebooks{}/BenchmarkApp.ipynb\".format(user,path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sending Email Notification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "# The subject line for the email.\n",
    "SUBJECT = \"Message from Intel® DevCloud for the Edge\"\n",
    "\n",
    "# The email body for recipients with non-HTML email clients.\n",
    "BODY_TEXT = (\"Completed benchmarking of your models. Please visit your Intel® DevCloud Notebook. \"\n",
    "            )\n",
    "            \n",
    "# The HTML body of the email.\n",
    "BODY_HTML = \"\"\"<html>\n",
    "<head></head>\n",
    "<body>\n",
    "  <h3> Message from Intel DevCloud </h3>\n",
    "  <p>Completed benchmarking of your models. Please visit your <a href={}>Intel® DevCloud Notebook</a>.</p>\n",
    "</body>\n",
    "</html>\n",
    "            \"\"\".format(notebook_url)         \n",
    "\n",
    "# The character encoding for the email.\n",
    "CHARSET = \"UTF-8\"\n",
    "\n",
    "# Create a new SES resource and specify a region.\n",
    "client = boto3.client('ses',region_name=AWS_REGION,aws_access_key_id=ACCESS_KEY_ID, aws_secret_access_key=SECRET_ACCESS_KEY)\n",
    "\n",
    "# Try to send the email.\n",
    "try:\n",
    "    #Provide the contents of the email.\n",
    "    response = client.send_email(\n",
    "        Destination={\n",
    "            'ToAddresses': [\n",
    "                RECIPIENT,\n",
    "            ],\n",
    "        },\n",
    "        Message={\n",
    "            'Body': {\n",
    "                'Html': {\n",
    "                    'Charset': CHARSET,\n",
    "                    'Data': BODY_HTML,\n",
    "                },\n",
    "                'Text': {\n",
    "                    'Charset': CHARSET,\n",
    "                    'Data': BODY_TEXT,\n",
    "                },\n",
    "            },\n",
    "            'Subject': {\n",
    "                'Charset': CHARSET,\n",
    "                'Data': SUBJECT,\n",
    "            },\n",
    "        },\n",
    "        Source=SENDER,\n",
    "        # If you are not using a configuration set, comment or delete the\n",
    "        # following line\n",
    "        # ConfigurationSetName=CONFIGURATION_SET,\n",
    "    )\n",
    "# Display an error if something goes wrong.\t\n",
    "except ClientError as e:\n",
    "    print(e.response['Error']['Message'])\n",
    "else:\n",
    "    print(\"Email sent! Message ID:\"),\n",
    "    print(response['MessageId'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next steps\n",
    "- [More Jupyter* Notebook Samples](https://devcloud.intel.com/edge/advanced/sample_applications/) - additional sample applications \n",
    "- [Jupyter* Notebook Tutorials](https://devcloud.intel.com/edge/get_started/tutorials) - sample application Jupyter* Notebook tutorials\n",
    "- [Intel® Distribution of OpenVINO™ toolkit Main Page](https://software.intel.com/openvino-toolkit) - learn more about the tools and use of the Intel® Distribution of OpenVINO™ toolkit for implementing inference on the edge\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About this notebook\n",
    "\n",
    "For technical support, please see the [Intel® DevCloud Forums](https://software.intel.com/en-us/forums/intel-devcloud-for-edge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=background-color:#0071C5;color:white;padding:0.5em;display:table-cell;width:100pc;vertical-align:middle>\n",
    "<img style=float:right src=\"https://devcloud.intel.com/edge/static/images/svg/IDZ_logo.svg\" alt=\"Intel DevCloud logo\" width=\"150px\"/>\n",
    "<a style=color:white>Intel® DevCloud for the Edge</a><br>   \n",
    "<a style=color:white href=\"#top\">Top of Page</a> | \n",
    "<a style=color:white href=\"https://devcloud.intel.com/edge/static/docs/terms/Intel-DevCloud-for-the-Edge-Usage-Agreement.pdf\">Usage Agreement (Intel)</a> | \n",
    "<a style=color:white href=\"https://devcloud.intel.com/edge/static/docs/terms/Colfax_Cloud_Service_Terms_v1.3.pdf\">Service Terms (Colfax)</a>\n",
    "</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (OpenVINO 2021.2)",
   "language": "python",
   "name": "c003-python_3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "213px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
